{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ssm cuda acceleration depends on mamba_ssm_paddle https://github.com/JunnYu/mamba.git\n",
    "# so you need install mamba_ssm_paddle first\n",
    "# git -clone -b mamba-v2.2.2 https://github.com/JunnYu/mamba.git\n",
    "# or you can get from ./ssm_paddle \n",
    "%cd mamba/ssm_paddle\n",
    "! pip install einops\n",
    "! python setup_causal_conv1d.py install\n",
    "! python setup_selective_scan.py install\n",
    "! sh install_all.sh\n",
    "%cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from typing import Optional, Union, Type, List, Tuple, Callable, Dict\n",
    "\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from mamba_ssm_paddle.ops.selective_scan_interface import selective_scan_fn, selective_scan_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this cell is SS2D paddle\n",
    "\n",
    "class SS2D(nn.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        d_state=16,\n",
    "        d_conv=3,\n",
    "        expand=2,\n",
    "        dt_rank=\"auto\",\n",
    "        dt_min=0.001,\n",
    "        dt_max=0.1,\n",
    "        dt_init=\"random\",\n",
    "        dt_scale=1.0,\n",
    "        dt_init_floor=1e-4,\n",
    "        dropout=0.,\n",
    "        conv_bias=True,\n",
    "        bias=False,\n",
    "    ):\n",
    "        super(SS2D, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.expand = expand\n",
    "        self.d_inner = int(self.expand * self.d_model)\n",
    "        self.dt_rank = math.ceil(self.d_model / 16) if dt_rank == \"auto\" else dt_rank\n",
    "\n",
    "        self.in_proj = nn.Linear(self.d_model, self.d_inner * 2, bias_attr=bias)\n",
    "        self.conv2d = nn.Conv2D(\n",
    "            in_channels=self.d_inner,\n",
    "            out_channels=self.d_inner,\n",
    "            groups=self.d_inner,\n",
    "            bias_attr=conv_bias,\n",
    "            kernel_size=d_conv,\n",
    "            padding=(d_conv - 1) // 2,\n",
    "        )\n",
    "        self.act = nn.Silu()\n",
    "\n",
    "        self.x_proj_layers = (\n",
    "            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias_attr=False),\n",
    "            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias_attr=False),\n",
    "            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias_attr=False),\n",
    "            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias_attr=False),\n",
    "        )\n",
    "        # Stack weights and delete the original layers\n",
    "        self.x_proj_weight = self.create_parameter(\n",
    "            shape=[4, self.dt_rank + self.d_state * 2, self.d_inner],\n",
    "            default_initializer=nn.initializer.Normal()  # Or use the appropriate initializer\n",
    "        )\n",
    "        self.x_proj_weights_temp = paddle.stack([layer.weight for layer in self.x_proj_layers], axis=0).transpose([0, 2, 1])\n",
    "        self.x_proj_weight.set_value(self.x_proj_weights_temp)\n",
    "        del self.x_proj_layers\n",
    "        del self.x_proj_weights_temp\n",
    "\n",
    "        self.dt_projs_layers = (\n",
    "            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor),\n",
    "            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor),\n",
    "            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor),\n",
    "            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor),\n",
    "        )\n",
    "        self.dt_projs_weight = self.create_parameter(\n",
    "            shape=[4, self.d_inner, self.dt_rank],\n",
    "            default_initializer=nn.initializer.Normal()  # Or use the appropriate initializer\n",
    "        )\n",
    "        self.dt_projs_weight_temp = paddle.stack([layer.weight for layer in self.dt_projs_layers], axis=0).transpose([0, 2, 1])\n",
    "        self.dt_projs_weight.set_value(self.dt_projs_weight_temp)\n",
    "        self.dt_projs_bias = self.create_parameter(\n",
    "            shape=[4, self.d_inner],\n",
    "            default_initializer=nn.initializer.Constant(0.0)  # Or use the appropriate initializer\n",
    "        )\n",
    "        \n",
    "        del self.dt_projs_layers\n",
    "        del self.dt_projs_weight_temp\n",
    "\n",
    "        self.A_logs = self.A_log_init(self.d_state, self.d_inner, copies=4, merge=True) # (K=4, D, N)\n",
    "        self.Ds = self.D_init(self.d_inner, copies=4, merge=True) # (K=4, D, N)\n",
    "\n",
    "        self.selective_scan = selective_scan_fn\n",
    "\n",
    "        self.out_norm = nn.LayerNorm(self.d_inner)\n",
    "        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias_attr=bias)\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0. else None\n",
    "\n",
    "    @staticmethod\n",
    "    def dt_init(dt_rank, d_inner, dt_scale=1.0, dt_init=\"random\", dt_min=0.001, dt_max=0.1, dt_init_floor=1e-4):\n",
    "        dt_proj = nn.Linear(dt_rank, d_inner, bias_attr=True)\n",
    "\n",
    "        # Initialize special dt projection to preserve variance at initialization\n",
    "        dt_init_std = dt_rank**-0.5 * dt_scale\n",
    "        if dt_init == \"constant\":\n",
    "            nn.initializer.Constant(dt_init_std)(dt_proj.weight)\n",
    "        elif dt_init == \"random\":\n",
    "            nn.initializer.Uniform(-dt_init_std, dt_init_std)(dt_proj.weight)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Initialize dt bias so that F.softplus(dt_bias) is between dt_min and dt_max\n",
    "        dt = paddle.exp(\n",
    "            paddle.rand([d_inner]) * (math.log(dt_max) - math.log(dt_min))\n",
    "            + math.log(dt_min)\n",
    "        ).clip(min=dt_init_floor)\n",
    "        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
    "        inv_dt = dt + paddle.log(-paddle.expm1(-dt))\n",
    "        with paddle.no_grad():\n",
    "            dt_proj.bias.set_value(inv_dt)\n",
    "        # Our initialization would set all Linear.bias to zero, need to mark this one as _no_reinit\n",
    "        dt_proj.bias._no_reinit = True\n",
    "\n",
    "        return dt_proj\n",
    "\n",
    "    @staticmethod\n",
    "    def A_log_init(d_state, d_inner, copies=1, device=None, merge=True):\n",
    "        # S4D real initialization\n",
    "        A = paddle.arange(1, d_state + 1, dtype='float32').unsqueeze(0).expand([d_inner, -1])\n",
    "        A_log = paddle.log(A)  # Keep A_log in fp32\n",
    "        if copies > 1:\n",
    "            A_log = A_log.unsqueeze(0).expand([copies, -1, -1])\n",
    "            if merge:\n",
    "                A_log = A_log.reshape([-1, d_state])\n",
    "        A_log = paddle.create_parameter(shape=A_log.shape, dtype='float32')\n",
    "        A_log._no_weight_decay = True\n",
    "        return A_log\n",
    "\n",
    "    @staticmethod\n",
    "    def D_init(d_inner, copies=1, device=None, merge=True):\n",
    "        # D \"skip\" parameter\n",
    "        D = paddle.ones([d_inner], dtype='float32')\n",
    "        if copies > 1:\n",
    "            D = D.unsqueeze(0).expand([copies, -1])\n",
    "            if merge:\n",
    "                D = D.reshape([-1])\n",
    "        D = paddle.create_parameter(shape=D.shape, dtype='float32')\n",
    "        D._no_weight_decay = True\n",
    "        return D\n",
    "\n",
    "    def forward_core(self, x: paddle.Tensor):\n",
    "        B, C, H, W = x.shape\n",
    "        L = H * W\n",
    "        K = 4\n",
    "\n",
    "        x_hwwh = paddle.stack([x.reshape([B, -1, L]), paddle.transpose(x, perm=[0, 2, 3, 1]).reshape([B, -1, L])], axis=1).reshape([B, 2, -1, L])\n",
    "        xs = paddle.concat([x_hwwh, paddle.flip(x_hwwh, axis=[-1])], axis=1) # (b, k, d, l)\n",
    "\n",
    "        x_dbl = paddle.einsum(\"b k d l, k c d -> b k c l\", xs.reshape([B, K, -1, L]), self.x_proj_weight)\n",
    "        dts, Bs, Cs = paddle.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], axis=2)\n",
    "        dts = paddle.einsum(\"b k r l, k d r -> b k d l\", dts.reshape([B, K, -1, L]), self.dt_projs_weight)\n",
    "\n",
    "        xs = xs.astype('float32').reshape([B, -1, L]) # (b, k * d, l)\n",
    "        dts = dts.reshape([B, -1, L]) # (b, k * d, l)\n",
    "        Bs = Bs.astype('float32').reshape([B, K, -1, L]) # (b, k, d_state, l)\n",
    "        Cs = Cs.astype('float32').reshape([B, K, -1, L]) # (b, k, d_state, l)\n",
    "        Ds = self.Ds.astype('float32').reshape([-1]) # (k * d)\n",
    "        As = -paddle.exp(self.A_logs.astype('float32')).reshape([-1, self.d_state])  # (k * d, d_state)\n",
    "        dt_projs_bias = self.dt_projs_bias.astype('float32').reshape([-1]) # (k * d)\n",
    "\n",
    "        out_y = self.selective_scan(\n",
    "            xs, dts,\n",
    "            As, Bs, Cs, Ds, z=None,\n",
    "            delta_bias=dt_projs_bias,\n",
    "            delta_softplus=True,\n",
    "            return_last_state=False,\n",
    "        ).reshape([B, K, -1, L])\n",
    "        assert out_y.dtype == paddle.float32\n",
    "\n",
    "        inv_y = paddle.flip(out_y[:, 2:4], axis=[-1]).reshape([B, 2, -1, L])\n",
    "        wh_y = paddle.transpose(out_y[:, 1].reshape([B, -1, W, H]), perm=[0, 1, 3, 2]).reshape([B, -1, L])\n",
    "        invwh_y = paddle.transpose(inv_y[:, 1].reshape([B, -1, W, H]), perm=[0, 1, 3, 2]).reshape([B, -1, L])\n",
    "\n",
    "        return out_y[:, 0], inv_y[:, 0], wh_y, invwh_y\n",
    "\n",
    "    def forward(self, x: paddle.Tensor):\n",
    "        B, H, W, C = x.shape\n",
    "\n",
    "        xz = self.in_proj(x)\n",
    "        x, z = paddle.split(xz, 2, axis=-1) # (b, h, w, d)\n",
    "\n",
    "        x = paddle.transpose(x, perm=[0, 3, 1, 2])\n",
    "        x = self.act(self.conv2d(x)) # (b, d, h, w)\n",
    "        print(x.shape)\n",
    "        y1, y2, y3, y4 = self.forward_core(x)\n",
    "        assert y1.dtype == paddle.float32\n",
    "        y = y1 + y2 + y3 + y4\n",
    "        print(y.shape)\n",
    "        y = paddle.transpose(y, perm=[0, 2, 1]).reshape([B, H, W, -1])\n",
    "        y = self.out_norm(y)\n",
    "        y = y * F.silu(z)\n",
    "        out = self.out_proj(y)\n",
    "        if self.dropout is not None:\n",
    "            out = self.dropout(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#this cell is PathcEmbed2d and PatchMerging2D\n",
    "\n",
    "class PatchEmbed2D(nn.Layer):\n",
    "    r\"\"\" Image to Patch Embedding\n",
    "    Args:\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Layer, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "    def __init__(self, patch_size=4, in_chans=1, embed_dim=96, norm_layer=None): # 修改通道数为1\n",
    "        super().__init__()\n",
    "        if isinstance(patch_size, int):\n",
    "            patch_size = (patch_size, patch_size)\n",
    "        self.proj = nn.Conv2D(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"dsm分割后的张量的尺寸(shape):\",x.shape)  # 或 tensor.size()\n",
    "        # print(\"dsm分割后的张量的维度数量(dim):\", x.dim())\n",
    "        x = self.proj(x).transpose([0, 2, 3, 1])\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchMerging2D(nn.Layer):\n",
    "    r\"\"\" Patch Merging Layer.\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Resolution of input feature.\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Layer, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias_attr=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, W, C = x.shape\n",
    "\n",
    "        SHAPE_FIX = [-1, -1]\n",
    "        if (W % 2 != 0) or (H % 2 != 0):\n",
    "            print(f\"Warning, x.shape {x.shape} is not match even ===========\", flush=True)\n",
    "            SHAPE_FIX[0] = H // 2\n",
    "            SHAPE_FIX[1] = W // 2\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "\n",
    "        if SHAPE_FIX[0] > 0:\n",
    "            x0 = x0[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n",
    "            x1 = x1[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n",
    "            x2 = x2[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n",
    "            x3 = x3[:, :SHAPE_FIX[0], :SHAPE_FIX[1], :]\n",
    "        \n",
    "        x = paddle.concat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.reshape([B, H//2, W//2, 4 * C])  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# VSSBackbone\n",
    "\n",
    "class VSSBlock(nn.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim: int = 0,\n",
    "        drop_path: float = 0,\n",
    "        norm_layer: Callable[..., paddle.nn.Layer] = partial(nn.LayerNorm, eps=1e-6),\n",
    "        attn_drop_rate: float = 0,\n",
    "        d_state: int = 16,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ln_1 = norm_layer(hidden_dim)\n",
    "        self.self_attention = SS2D(d_model=hidden_dim, dropout=attn_drop_rate, d_state=d_state, **kwargs)\n",
    "        self.drop_path = nn.Dropout(drop_path)\n",
    "\n",
    "    def forward(self, input: paddle.Tensor):\n",
    "        x = input + self.drop_path(self.self_attention(self.ln_1(input)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class VSSLayer(nn.Layer):\n",
    "    \"\"\" A basic layer for one stage.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        depth (int): Number of blocks.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Layer, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Layer | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        dim, \n",
    "        depth, \n",
    "        attn_drop=0.,\n",
    "        drop_path=0., \n",
    "        norm_layer=nn.LayerNorm, \n",
    "        downsample=None, \n",
    "        use_checkpoint=False, \n",
    "        d_state=16,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        self.blocks = nn.LayerList([\n",
    "            VSSBlock(\n",
    "                hidden_dim=dim,\n",
    "                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                norm_layer=norm_layer,\n",
    "                attn_drop_rate=attn_drop,\n",
    "                d_state=d_state,\n",
    "            )\n",
    "            for i in range(depth)])\n",
    "        \n",
    "        if True: # is this really applied? Yes, but been overriden later in VSSM!\n",
    "            def _init_weights(module: nn.Layer):\n",
    "                for name, p in module.named_parameters():\n",
    "                    if name in [\"out_proj.weight\"]:\n",
    "                        p = p.clone().detach_() # fake init, just to keep the seed ....\n",
    "                        nn.initializer.KaimingUniform(math.sqrt(5))(p)\n",
    "            self.apply(_init_weights)\n",
    "\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = paddle.load(blk, x)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class VSSBackbone(nn.Layer):\n",
    "    def __init__(self, patch_size=4, in_chans=3, depths=[2, 2, 9, 2], \n",
    "                 dims=[96, 192, 384, 768], d_state=16, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.2, \n",
    "                 norm_layer=nn.LayerNorm, patch_norm=True, \n",
    "                 use_checkpoint=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_layers = len(depths)\n",
    "        if isinstance(dims, int):\n",
    "            dims = [int(dims * 2 ** i_layer) for i_layer in range(self.num_layers)]\n",
    "        self.embed_dim = dims[0]\n",
    "        self.num_features = dims[-1]\n",
    "        self.dims = dims\n",
    " \n",
    "        # PatchEmbed2D\n",
    "        self.patch_embed = PatchEmbed2D(patch_size=patch_size, in_chans=in_chans, embed_dim=self.embed_dim,\n",
    "            norm_layer=norm_layer if patch_norm else None) \n",
    "\n",
    "        # WASTED absolute position embedding ======================\n",
    "        self.ape = False\n",
    "        if self.ape:\n",
    "            self.patches_resolution = self.patch_embed.patches_resolution\n",
    "            # self.absolute_pos_embed = nn.Parameter(paddle.zeros(1, *self.patches_resolution, self.embed_dim))\n",
    "            # trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "            self.absolute_pos_embed = paddle.ParamAttr(initializer=paddle.nn.initializer.TruncatedNormal(std=.02))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [x.item() for x in paddle.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        self.layers = nn.LayerList()\n",
    "        self.downsamples = nn.LayerList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = VSSLayer(\n",
    "                dim=dims[i_layer],\n",
    "                depth=depths[i_layer],\n",
    "                d_state=math.ceil(dims[0] / 6) if d_state is None else d_state, # 20240109\n",
    "                drop=drop_rate, \n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                norm_layer=norm_layer,\n",
    "                downsample=None,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "            if i_layer < self.num_layers - 1:\n",
    "                self.downsamples.append(PatchMerging2D(dim=dims[i_layer], norm_layer=norm_layer))\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m: nn.Layer):\n",
    "        \"\"\"\n",
    "        out_proj.weight which is previously initilized in VSSBlock, would be cleared in nn.Linear\n",
    "        no fc.weight found in the any of the model parameters\n",
    "        no nn.Embedding found in the any of the model parameters\n",
    "        so the thing is, VSSBlock initialization is useless\n",
    "        \n",
    "        Conv2D is not intialized !!!\n",
    "        \"\"\"\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.initializer.Normal(std=.02)(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.initializer.Constant(value=0.)(m.bias)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.initializer.Constant(value=0.)(m.bias)\n",
    "            nn.initializer.Constant(value=1.)(m.weight)\n",
    "\n",
    "    def no_weight_decay(self):\n",
    "        return {'absolute_pos_embed'}\n",
    "    \n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'relative_position_bias_table'}\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_ret = []\n",
    "        x_ret.append(x)\n",
    "\n",
    "        x = self.patch_embed(x)\n",
    "        print('分割后的的rgb:',x.shape)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for s, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            x_ret.append(x.transpose([0, 3, 1, 2]))\n",
    "            if s < len(self.downsamples):\n",
    "                x = self.downsamples[s](x)\n",
    "\n",
    "        return x_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\r\n",
      "  warnings.warn(warning_message)\r\n",
      "test SS2D\r\n",
      "W1116 18:52:26.856954 58892 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.8\r\n",
      "W1116 18:52:26.858069 58892 gpu_resources.cc:164] device: 0, cuDNN Version: 8.9.\r\n",
      "input shape is  [1, 64, 64, 128]\r\n",
      "output shape is  [1, 64, 64, 128]\r\n"
     ]
    }
   ],
   "source": [
    "# SS2D depends on SwinUMamba https://github.com/JiarunLiu/Swin-UMamba/blob/main/swin_umamba/nnunetv2/nets/SwinUMamba.py\n",
    "! python test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "import paddle \n",
    "from vmamba import SS2D, VSSBackbone\n",
    "\n",
    "x = paddle.rand([1, 3,512, 512]).cuda()\n",
    "print(\"input shape is \", x.shape)\n",
    "m = VSSBackbone().to('gpu:0')\n",
    "y = m(x)\n",
    "print(\"outputs shape are \")\n",
    "for i in y:\n",
    "    print(i.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('python35-paddle120-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "09f0dbf7b1569c1ab842ae2f41770fe6aa1b54326d081112fa5944b99abb5899"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
